% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kNN.classification.R
\name{kNN.classification}
\alias{kNN.classification}
\title{k Nearest Neighbours Classification}
\usage{
kNN.classification(df, DeltaGoF.emp, k, ties = "model2",
  verbose = TRUE)
}
\arguments{
\item{df}{Results of bootstrap; the output of \code{\link{pbcm.di}} or \code{\link{pbcm.du}}}

\item{DeltaGoF.emp}{Empirical value of goodness of fit (e.g. from \code{\link{empirical.GoF}})}

\item{k}{Number of neighbours to employ in classification; may be a vector of integers}

\item{ties}{Which way should ties (when distance to the two distributions is equal) be broken? By default, we break in favour of model 2, taking this to be the null model in the comparison.}

\item{verbose}{If \code{TRUE}, warnings are issued to the console}
}
\value{
A data frame containing the computed distances and decisions, one row per each value of \code{k}
}
\description{
Carry out k Nearest Neighbours (k-NN) classification on the results of a parametric boostrap.
}
\details{
Calculated the cumulative distance (sum of squared differences) of \code{DeltaGoF.emp} to both \code{DeltaGoF} distributions found in \code{df} (i.e. one with model 1 as generator and one with model 2 as generator), taking into account the \code{k} nearest neighbours only. Decides in favour of model 1 if this cumulative distance to the model 1 distribution is smaller than than the distance to model 2, and vice versa. If distances are equal, decision is made according to the \code{ties} argument.
}
\references{
Schultheis, H. & Singhaniya, A. (2015) Decision criteria for model comparison using the parametric bootstrap cross-fitting method. \emph{Cognitive Systems Research}, 33, 100-121.
}
\seealso{
\code{\link{empirical.GoF}}, \code{\link{pbcm.di}}, \code{\link{pbcm.du}}
}
\author{
Henri Kauhanen
}
