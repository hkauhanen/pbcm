% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kNN.confusionmatrix.R
\name{kNN.confusionmatrix}
\alias{kNN.confusionmatrix}
\title{Confusion Matrices through k Nearest Neighbours Classification}
\usage{
kNN.confusionmatrix(
  df,
  df.holdout,
  k,
  ties = "model2",
  print_genargs = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{df}{Data frame output by \code{\link{pbcm.di}} or \code{\link{pbcm.du}}}

\item{df.holdout}{Data frame output by \code{\link{pbcm.di}} or \code{\link{pbcm.du}}}

\item{k}{Number of neighbours to consider in k-NN classification; may be a vector of integers}

\item{ties}{Which way to break ties in k-NN classification (see \code{\link{kNN.classification}})}

\item{print_genargs}{Should the generator arguments of the holdout distribution be included in the output? (See Details)}

\item{verbose}{If \code{TRUE}, prints a progress bar and issues warnings}
}
\value{
A data frame with the following columns:
\describe{
\item{\code{k}}{Number of nearest neighbours}
\item{\code{P}}{Number of positives}
\item{\code{N}}{Number of negatives}
\item{\code{TP}}{Number of true positives}
\item{\code{FP}}{Number of false positives}
\item{\code{TN}}{Number of true negatives}
\item{\code{FN}}{Number of false negatives}
\item{\code{alpha}}{Type I error (false positive) rate; equal to \code{FP} divided by \code{N}}
\item{\code{beta}}{Type II error (false negative) rate; equal to \code{FN}Â divided by \code{P}}
}
In addition to these columns, if \code{print_genargs == TRUE}, each argument that was passed via \code{genargs1} and \code{genargs2} to \code{\link{pbcm.di}} or \code{\link{pbcm.du}} to generate \code{df.holdout} is included as a column of its own.
}
\description{
Computes confusion matrices (one for each value of \eqn{k}) using \eqn{k}-NN classification from the results of two parametric bootstraps, one of these being labelled a holdout set and tested against the other one.
}
\details{
The function takes each \code{DeltaGoF} value from \code{df.holdout}, compares it against the \code{DeltaGoF} distributions in \code{df}, and decides based on \eqn{k}-NN classification. By convention, we take model 2 as the null hypothesis and model 1 as the alternative. Hence a false positive, for instance, means the situation where model 2 generated the data but the decision was in favour of model 1.
}
\examples{
x <- seq(from=0, to=1, length.out=100)
mockdata <- data.frame(x=x, y=x + rnorm(100, 0, 0.5))

myfitfun <- function(data, p) {
  res <- nls(y~a*x^p, data, start=list(a=1.1))
  list(a=coef(res), GoF=deviance(res))
}

mygenfun <- function(model, p) { 
  x <- seq(from=0, to=1, length.out=100)
  y <- model$a*x^p + rnorm(100, 0, 0.5)
  data.frame(x=x, y=y)
}

pb1 <- pbcm.di(data=mockdata, fun1=myfitfun, fun2=myfitfun, genfun1=mygenfun,
        genfun2=mygenfun, reps=20, args1=list(p=1), args2=list(p=2), 
        genargs1=list(p=1), genargs2=list(p=2))

pb2 <- pbcm.di(data=mockdata, fun1=myfitfun, fun2=myfitfun, genfun1=mygenfun,
        genfun2=mygenfun, reps=20, args1=list(p=1), args2=list(p=2), 
        genargs1=list(p=1), genargs2=list(p=2))

kNN.confusionmatrix(df=pb1, df.holdout=pb2, k=1:10)
}
\seealso{
\code{\link{kNN.classification}}, \code{\link{pbcm.di}}, \code{\link{pbcm.du}}
}
\author{
Henri Kauhanen
}
